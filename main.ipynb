{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e425e8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is intentionally left blank. See the new notebook structure below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5ff63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is intentionally left blank. See the new notebook structure below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbb7a9c",
   "metadata": {},
   "source": [
    "# Predictive Modelling of Eating-Out Problem\n",
    "\n",
    "## Assignment Overview\n",
    "\n",
    "This notebook addresses the requirements for the Predictive Modelling of Eating-Out Problem assignment. We will perform exploratory data analysis (EDA), build regression and classification models using Scikit-Learn and PySpark MLlib, and ensure reproducibility using Git, Git LFS, and DVC. The dataset is 'zomato_df_final_data.csv' with restaurant details from Sydney (2018), and 'sydney.geojson' for geospatial analysis.\n",
    "\n",
    "\n",
    "\n",
    "### Objectives\n",
    "\n",
    "- Perform EDA and visualize key insights.\n",
    "\n",
    "- Build and evaluate regression and classification models.\n",
    "\n",
    "- Use PySpark MLlib for scalable modeling.\n",
    "\n",
    "- Ensure reproducibility with Git, Git LFS, and DVC.\n",
    "\n",
    "\n",
    "\n",
    "**Note**: Run this notebook in an environment with required libraries installed (see requirements.txt). Replace file paths if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5926c67a",
   "metadata": {},
   "source": [
    "## Deliverables\n",
    "- This notebook with code and outputs.\n",
    "- PDF report summarizing findings (export this notebook to PDF).\n",
    "- GitHub link: <insert_link>\n",
    "\n",
    "### Reflection\n",
    "PySpark is better for scalability but Scikit-Learn is simpler for small data. Models perform reasonably; Random Forest had best F1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96aae52c",
   "metadata": {},
   "source": [
    "### Git & GitHub\n",
    "Initialize Git:\n",
    "```\n",
    "git init\n",
    "git lfs install\n",
    "git lfs track \"*.csv\" \"*.geojson\" \"*.pkl\"\n",
    "git add .\n",
    "git commit -m \"Initial commit\"\n",
    "git remote add origin <github_repo_url>\n",
    "git push -u origin main\n",
    "```\n",
    "\n",
    "### README.md\n",
    "- Install dependencies: pip install -r requirements.txt\n",
    "- Run: jupyter notebook EatingOut_Analysis.ipynb\n",
    "- Expected: EDA plots, model MSE/F1 scores.\n",
    "\n",
    "### Data Version Control (DVC)\n",
    "```\n",
    "dvc init\n",
    "dvc add zomato_df_final_data.csv sydney.geojson\n",
    "dvc remote add -d storage <remote>\n",
    "dvc push\n",
    "\n",
    "# dvc.yaml\n",
    "stages:\n",
    "  preprocess:\n",
    "    cmd: python preprocess.py\n",
    "    deps:\n",
    "      - zomato_df_final_data.csv\n",
    "    outs:\n",
    "      - processed_data.csv\n",
    "  model:\n",
    "    cmd: python model.py\n",
    "    deps:\n",
    "      - processed_data.csv\n",
    "    outs:\n",
    "      - model.pkl\n",
    "  evaluate:\n",
    "    cmd: python evaluate.py\n",
    "    deps:\n",
    "      - model.pkl\n",
    "      - processed_data.csv\n",
    "```\n",
    "Run: dvc repro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58df69a1",
   "metadata": {},
   "source": [
    "## Part C â€“ Reproducibility and Workflow (10 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d65214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Spark session\n",
    "\n",
    "spark = SparkSession.builder.appName('EatingOut').getOrCreate()\n",
    "\n",
    "spark_df = spark.createDataFrame(df)\n",
    "\n",
    "\n",
    "\n",
    "# Features\n",
    "\n",
    "assembler = VectorAssembler(inputCols=['cost', 'votes', 'subzone_encoded', 'cuisine_diversity'], outputCol='features')\n",
    "\n",
    "\n",
    "\n",
    "# Regression with PySpark\n",
    "\n",
    "lr_spark = SparkLinearRegression(featuresCol='features', labelCol='rating_number')\n",
    "\n",
    "pipeline_reg = SparkPipeline(stages=[assembler, lr_spark])\n",
    "\n",
    "train, test = spark_df.randomSplit([0.8, 0.2])\n",
    "\n",
    "model_reg = pipeline_reg.fit(train)\n",
    "\n",
    "pred_reg = model_reg.transform(test)\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol='rating_number', metricName='mse')\n",
    "\n",
    "mse_spark = evaluator.evaluate(pred_reg)\n",
    "\n",
    "print('Spark MSE:', mse_spark)\n",
    "\n",
    "\n",
    "\n",
    "# Classification with PySpark\n",
    "\n",
    "log_reg_spark = SparkLogisticRegression(featuresCol='features', labelCol='class')\n",
    "\n",
    "pipeline_class = SparkPipeline(stages=[assembler, log_reg_spark])\n",
    "\n",
    "model_class = pipeline_class.fit(train)\n",
    "\n",
    "pred_class = model_class.transform(test)\n",
    "\n",
    "evaluator_class = MulticlassClassificationEvaluator(labelCol='class', metricName='f1')\n",
    "\n",
    "f1_spark = evaluator_class.evaluate(pred_class)\n",
    "\n",
    "print('Spark F1:', f1_spark)\n",
    "\n",
    "\n",
    "\n",
    "# Comparison: PySpark is more scalable for large data, but similar accuracy; Scikit-Learn is faster for small datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad1ddce",
   "metadata": {},
   "source": [
    "### 4. PySpark Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96339504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplify rating_text to binary\n",
    "\n",
    "df['class'] = df['rating_text'].apply(lambda x: 1 if x in ['Poor', 'Average'] else 2 if x in ['Good', 'Very Good', 'Excellent'] else 0)\n",
    "\n",
    "y_class = df['class']\n",
    "\n",
    "\n",
    "\n",
    "# Split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_class, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# Logistic Regression\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "y_pred_log = log_reg.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred_log))\n",
    "\n",
    "print('Precision:', precision_score(y_test, y_pred_log, average='weighted'))\n",
    "\n",
    "print('Recall:', recall_score(y_test, y_pred_log, average='weighted'))\n",
    "\n",
    "print('F1:', f1_score(y_test, y_pred_log, average='weighted'))\n",
    "\n",
    "\n",
    "\n",
    "# Other models\n",
    "\n",
    "models = {\n",
    "\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "\n",
    "    'Gradient Boosted Trees': GradientBoostingClassifier(),\n",
    "\n",
    "    'SVM': SVC(),\n",
    "\n",
    "    'Neural Net': MLPClassifier()\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    results.append({\n",
    "\n",
    "        'Model': name,\n",
    "\n",
    "        'Precision': precision_score(y_test, y_pred, average='weighted'),\n",
    "\n",
    "        'Recall': recall_score(y_test, y_pred, average='weighted'),\n",
    "\n",
    "        'F1': f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "# Compare in table\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76db5e1",
   "metadata": {},
   "source": [
    "### 3. Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a370792b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features and target\n",
    "\n",
    "X = df[['cost', 'votes', 'subzone_encoded', 'cuisine_diversity']]\n",
    "\n",
    "y_reg = df['rating_number']\n",
    "\n",
    "\n",
    "\n",
    "# Split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_reg, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# Model A: Linear Regression\n",
    "\n",
    "lr = LinearRegression()\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "\n",
    "mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
    "\n",
    "print('MSE Linear Regression:', mse_lr)\n",
    "\n",
    "\n",
    "\n",
    "# Model B: Gradient Descent Regression (using SGDRegressor)\n",
    "\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "sgd = SGDRegressor(max_iter=1000, tol=1e-3)\n",
    "\n",
    "sgd.fit(X_train, y_train)\n",
    "\n",
    "y_pred_sgd = sgd.predict(X_test)\n",
    "\n",
    "mse_sgd = mean_squared_error(y_test, y_pred_sgd)\n",
    "\n",
    "print('MSE Gradient Descent:', mse_sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3f49c5",
   "metadata": {},
   "source": [
    "### 2. Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62a1631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values: drop rows with missing target, impute others\n",
    "\n",
    "df = df.dropna(subset=['rating_number', 'rating_text'])\n",
    "\n",
    "df['cost'] = df['cost'].fillna(df['cost'].median())\n",
    "\n",
    "df['votes'] = df['votes'].fillna(df['votes'].median())\n",
    "\n",
    "\n",
    "\n",
    "# Justify: Dropping missing targets as they are essential; median impute for numeric to avoid bias.\n",
    "\n",
    "\n",
    "\n",
    "# Encode categorical\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "df['subzone_encoded'] = le.fit_transform(df['subzone'].astype(str))\n",
    "\n",
    "\n",
    "\n",
    "# Create features: cuisine diversity\n",
    "\n",
    "df['cuisine_diversity'] = df['cuisine'].apply(len)\n",
    "\n",
    "df['cost_bin'] = pd.cut(df['cost'], bins=[0, 50, 100, 150, np.inf], labels=['Low', 'Medium', 'High', 'Very High'])\n",
    "\n",
    "\n",
    "\n",
    "# One-hot for cost_bin\n",
    "\n",
    "df = pd.get_dummies(df, columns=['cost_bin'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e686403",
   "metadata": {},
   "source": [
    "### 1. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06e7c5a",
   "metadata": {},
   "source": [
    "## Part B â€“ Predictive Modelling (20 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7c065b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive scatter plot for cost vs rating\n",
    "\n",
    "fig = px.scatter(df, x='cost', y='rating_number', color='rating_text', hover_data=['subzone'])\n",
    "\n",
    "fig.show()\n",
    "\n",
    "\n",
    "\n",
    "# Explanation: Interactive version allows hovering for details and zooming, better than static for exploring outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb7ebee",
   "metadata": {},
   "source": [
    "### Interactive Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d81b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load geojson\n",
    "\n",
    "gdf = gpd.read_file('sydney.geojson')\n",
    "\n",
    "\n",
    "\n",
    "# Choose a cuisine, e.g., 'Japanese'\n",
    "\n",
    "df['has_japanese'] = df['cuisine'].apply(lambda x: 1 if 'Japanese' in x else 0)\n",
    "\n",
    "suburb_counts = df.groupby('subzone')['has_japanese'].sum().reset_index()\n",
    "\n",
    "suburb_counts.columns = ['SSC_NAME', 'count']\n",
    "\n",
    "\n",
    "\n",
    "# Merge with geo\n",
    "\n",
    "merged = gdf.merge(suburb_counts, on='SSC_NAME', how='left').fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "# Plot\n",
    "\n",
    "merged.plot(column='count', cmap='OrRd', legend=True)\n",
    "\n",
    "plt.title('Japanese Cuisine Density per Suburb')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c903666",
   "metadata": {},
   "source": [
    "### Geospatial Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d33a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of cost, ratings, restaurant types\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "sns.histplot(df['cost'], ax=axes[0])\n",
    "\n",
    "axes[0].set_title('Cost Distribution')\n",
    "\n",
    "sns.histplot(df['rating_number'], ax=axes[1])\n",
    "\n",
    "axes[1].set_title('Rating Distribution')\n",
    "\n",
    "df['type'] = df['type'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else [])\n",
    "\n",
    "all_types = [t for sublist in df['type'] for t in sublist]\n",
    "\n",
    "sns.countplot(y=all_types, ax=axes[2])\n",
    "\n",
    "axes[2].set_title('Restaurant Types')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Correlation between cost and votes\n",
    "\n",
    "sns.scatterplot(x='cost', y='votes', data=df)\n",
    "\n",
    "plt.title('Correlation between Cost and Votes')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print('Correlation:', df['cost'].corr(df['votes']))\n",
    "\n",
    "\n",
    "\n",
    "# Insights: Higher cost restaurants tend to have more votes, indicating popularity. Casual Dining is the most common type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71db2d0e",
   "metadata": {},
   "source": [
    "### Explore Key Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2ca0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse cuisine lists\n",
    "\n",
    "df['cuisine'] = df['cuisine'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else [])\n",
    "\n",
    "\n",
    "\n",
    "# How many unique cuisines?\n",
    "\n",
    "all_cuisines = [c for sublist in df['cuisine'] for c in sublist]\n",
    "\n",
    "unique_cuisines = set(all_cuisines)\n",
    "\n",
    "print(f'Number of unique cuisines: {len(unique_cuisines)}')\n",
    "\n",
    "\n",
    "\n",
    "# Top 3 suburbs with most restaurants\n",
    "\n",
    "top_suburbs = df['subzone'].value_counts().head(3)\n",
    "\n",
    "print('Top 3 suburbs:', top_suburbs)\n",
    "\n",
    "\n",
    "\n",
    "# Plot top suburbs\n",
    "\n",
    "sns.barplot(x=top_suburbs.index, y=top_suburbs.values)\n",
    "\n",
    "plt.title('Top 3 Suburbs with Most Restaurants')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Are 'Excellent' ratings more expensive than 'Poor'?\n",
    "\n",
    "excellent_cost = df[df['rating_text'] == 'Excellent']['cost']\n",
    "\n",
    "poor_cost = df[df['rating_text'] == 'Poor']['cost']\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "sns.histplot(excellent_cost, ax=ax[0])\n",
    "\n",
    "ax[0].set_title('Cost Distribution for Excellent Ratings')\n",
    "\n",
    "sns.histplot(poor_cost, ax=ax[1])\n",
    "\n",
    "ax[1].set_title('Cost Distribution for Poor Ratings')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Insights: Excellent rated restaurants tend to have higher average costs than Poor rated ones, as seen in the histograms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bdae38",
   "metadata": {},
   "source": [
    "### Answer Specific Questions with Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07610ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "\n",
    "df = pd.read_csv('zomato_df_final_data.csv')\n",
    "\n",
    "\n",
    "\n",
    "# Check missing values, data types, and summary statistics\n",
    "\n",
    "print(df.info())\n",
    "\n",
    "print(df.describe())\n",
    "\n",
    "print(df.isnull().sum())\n",
    "\n",
    "\n",
    "\n",
    "# Insights: The dataset has over 10,000 records with some missing values in rating_number, lat, lng, etc. We will handle them in feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b20a508",
   "metadata": {},
   "source": [
    "### Load and Explore the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18ea78c",
   "metadata": {},
   "source": [
    "## Part A â€“ Exploratory Data Analysis (20 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b26e42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas matplotlib seaborn geopandas plotly scikit-learn pyspark dvc\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import geopandas as gpd\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder as SparkOneHotEncoder\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression as SparkLinearRegression\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression as SparkLogisticRegression\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "from pyspark.ml import Pipeline as SparkPipeline\n",
    "\n",
    "import ast\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b615ec7c",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Install required libraries if needed and import them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
